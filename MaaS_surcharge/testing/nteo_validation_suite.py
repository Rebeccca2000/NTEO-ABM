# nteo_validation_suite.py
"""
NTEO Validation Suite - Comprehensive Testing Framework
Validates the transition to unified network topology system and eliminates dual routing.
"""

import time
import traceback
import numpy as np
import networkx as nx
from typing import Dict, List, Any, Tuple
from pathlib import Path
import json
import warnings

# Import all components of the new system
from config.network_config import NetworkConfigurationManager, get_current_config, switch_network_type
from config.network_factory import NetworkFactory, create_network, batch_create_networks
from abm_initialization import create_nteo_model, MobilityModelNTEO
from research.nteo_research_runner import NTEOResearchRunner
import config.database_updated as db

class NTEOValidationSuite:
    """
    Comprehensive validation suite for the unified NTEO system.
    Tests all components and validates the elimination of dual routing.
    """
    
    def __init__(self, output_dir: str = "validation_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.test_results = {}
        self.performance_metrics = {}
        self.validation_errors = []
        
        print("üß™ NTEO Validation Suite Initialized")
        print(f"üìÅ Output directory: {self.output_dir}")
    
    def run_full_validation(self) -> Dict[str, Any]:
        """Run complete validation suite"""
        
        print("\n" + "="*80)
        print("NTEO UNIFIED SYSTEM VALIDATION SUITE")
        print("="*80)
        print("üéØ Objective: Validate transition to single network topology system")
        print("üö´ Critical: Confirm elimination of dual routing system")
        print("‚úÖ Success: 50%+ performance improvement expected")
        
        validation_results = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'tests_run': 0,
            'tests_passed': 0,
            'tests_failed': 0,
            'critical_issues': [],
            'performance_improvements': {},
            'detailed_results': {}
        }
        
        # Test Categories
        test_categories = [
            ('Configuration System', self._test_configuration_system),
            ('Network Factory', self._test_network_factory),
            ('ABM Integration', self._test_abm_integration),
            ('Performance Validation', self._test_performance_metrics),
            ('Research Framework', self._test_research_framework),
            ('Routing System', self._test_single_routing_system),
            ('Backward Compatibility', self._test_backward_compatibility),
            ('Network Topology Validation', self._test_network_topologies),
            ('Data Consistency', self._test_data_consistency),
            ('Error Handling', self._test_error_handling)
        ]
        
        for category_name, test_function in test_categories:
            print(f"\n{'='*60}")
            print(f"TESTING: {category_name.upper()}")
            print('='*60)
            
            try:
                category_results = test_function()
                validation_results['detailed_results'][category_name] = category_results
                
                # Count test results
                passed = category_results.get('tests_passed', 0)
                failed = category_results.get('tests_failed', 0)
                
                validation_results['tests_run'] += passed + failed
                validation_results['tests_passed'] += passed
                validation_results['tests_failed'] += failed
                
                # Check for critical issues
                if category_results.get('critical_issues'):
                    validation_results['critical_issues'].extend(category_results['critical_issues'])
                
                print(f"‚úÖ {category_name}: {passed} passed, {failed} failed")
                
            except Exception as e:
                error_msg = f"{category_name} test suite failed: {str(e)}"
                validation_results['critical_issues'].append(error_msg)
                validation_results['tests_failed'] += 1
                print(f"‚ùå {category_name}: SUITE FAILURE - {error_msg}")
        
        # Calculate overall results
        total_tests = validation_results['tests_run']
        passed_tests = validation_results['tests_passed']
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        print(f"\n{'='*80}")
        print("VALIDATION SUITE SUMMARY")
        print('='*80)
        print(f"üìä Total Tests: {total_tests}")
        print(f"‚úÖ Passed: {passed_tests}")
        print(f"‚ùå Failed in run_full_validation: {validation_results['tests_failed']}")
        print(f"üìà Success Rate: {success_rate:.1f}%")
        print(f"üö® Critical Issues: {len(validation_results['critical_issues'])}")
        
        if validation_results['critical_issues']:
            print("\nüö® CRITICAL ISSUES FOUND:")
            for issue in validation_results['critical_issues']:
                print(f"   - {issue}")
        
        validation_results['success_rate'] = success_rate
        validation_results['overall_status'] = 'PASS' if success_rate >= 80 and len(validation_results['critical_issues']) == 0 else 'FAIL'
        
        # Save validation results
        self._save_validation_results(validation_results)
        
        return validation_results
    
    def _test_configuration_system(self) -> Dict[str, Any]:
        """Test the unified configuration system"""
        
        results = {'tests_passed': 0, 'tests_failed': 0, 'critical_issues': []}
        
        # Test 1: Configuration Manager Creation
        print("üß™ Test 1: Configuration Manager Creation")
        try:
            config_manager = NetworkConfigurationManager()
            base_config = config_manager.get_base_configuration()
            
            assert 'topology_type' in base_config
            assert 'grid_width' in base_config
            assert 'grid_height' in base_config
            
            print("   ‚úÖ Configuration manager creates valid base configuration")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Configuration manager creation failed: {e}")
            results['tests_failed'] += 1
            results['critical_issues'].append("Configuration manager creation failed")
        
        # Test 2: Topology Type Switching
        print("üß™ Test 2: Topology Type Switching")
        try:
            config_manager = NetworkConfigurationManager()
            original_type = config_manager.master_topology_type
            
            config_manager.switch_topology_type('small_world')
            assert config_manager.master_topology_type == 'small_world'
            
            config_manager.switch_topology_type(original_type)
            assert config_manager.master_topology_type == original_type
            
            print("   ‚úÖ Topology switching works correctly")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Topology switching failed: {e}")
            results['tests_failed'] += 1
        
        # Test 3: Parameter Range Validation
        print("üß™ Test 3: Parameter Range Validation")
        try:
            config_manager = NetworkConfigurationManager()
            
            for topology in ['degree_constrained', 'small_world', 'scale_free']:
                config_manager.switch_topology_type(topology)
                param_range = config_manager.get_parameter_range()
                
                assert len(param_range) > 0, f"No parameters for {topology}"
                assert all(isinstance(p, (int, float)) for p in param_range), f"Invalid parameter types for {topology}"
            
            print("   ‚úÖ Parameter ranges valid for all topology types")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Parameter range validation failed: {e}")
            results['tests_failed'] += 1
        
        # Test 4: Research Configuration Access
        print("üß™ Test 4: Research Configuration Access")
        try:
            from config.network_config import get_research_config
            
            research_config = get_research_config('degree_comparison_study')
            
            assert 'topology_type' in research_config
            assert 'parameter_range' in research_config
            assert 'analysis_description' in research_config
            
            print("   ‚úÖ Research configurations accessible")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Research configuration access failed: {e}")
            results['tests_failed'] += 1
        
        return results
    
    def _test_network_factory(self) -> Dict[str, Any]:
        """Test the network factory pattern"""
        
        results = {'tests_passed': 0, 'tests_failed': 0, 'critical_issues': []}
        
        # Test 1: Factory Creation
        print("üß™ Test 1: Network Factory Creation")
        try:
            factory = NetworkFactory()
            assert hasattr(factory, 'create_network')
            assert hasattr(factory, 'config_manager')
            
            print("   ‚úÖ Network factory creates successfully")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Network factory creation failed: {e}")
            results['tests_failed'] += 1
            results['critical_issues'].append("Network factory creation failed")
        
        # Test 2: Degree-Constrained Network Creation
        print("üß™ Test 2: Degree-Constrained Network Creation")
        try:
            network_interface = create_network('degree_constrained', 4)
            stats = network_interface.get_network_stats()
            
            assert stats['num_nodes'] > 0
            assert stats['num_edges'] > 0
            assert stats['is_connected']
            
            print(f"   ‚úÖ Degree-4 network: {stats['num_nodes']} nodes, {stats['num_edges']} edges")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Degree-constrained network creation failed: {e}")
            results['tests_failed'] += 1
        
        # Test 3: Small-World Network Creation (if available)
        print("üß™ Test 3: Small-World Network Creation")
        try:
            network_interface = create_network('small_world', 0.1)
            stats = network_interface.get_network_stats()
            
            assert stats['num_nodes'] > 0
            assert stats['num_edges'] > 0
            
            print(f"   ‚úÖ Small-world network: {stats['num_nodes']} nodes, {stats['num_edges']} edges")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Small-world network creation skipped: {e}")
            # Don't count as failure - may not be implemented yet
            
        # Test 4: Batch Network Creation
        print("üß™ Test 4: Batch Network Creation")
        try:
            batch_networks = batch_create_networks('degree_constrained', [3, 4, 5])
            
            assert len(batch_networks) >= 2  # At least some should succeed
            
            for net_name, net_interface in batch_networks.items():
                stats = net_interface.get_network_stats()
                assert stats['num_nodes'] > 0
            
            print(f"   ‚úÖ Batch creation: {len(batch_networks)} networks created")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Batch network creation failed: {e}")
            results['tests_failed'] += 1
        
        return results
    
    def _test_abm_integration(self) -> Dict[str, Any]:
        """Test ABM integration with unified system"""
        
        results = {'tests_passed': 0, 'tests_failed': 0, 'critical_issues': []}
        
        # Test 1: NTEO Model Creation
        print("üß™ Test 1: NTEO Model Creation")
        try:
            model = create_nteo_model(
                topology_type='degree_constrained',
                variation_parameter=4,
                num_commuters=20  # Small for testing
            )
            
            assert hasattr(model, 'network_interface')
            assert hasattr(model, 'network_manager')
            assert model.num_commuters == 20
            
            print("   ‚úÖ NTEO model creates successfully")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå NTEO model creation failed: {e}")
            results['tests_failed'] += 1
            results['critical_issues'].append("NTEO model creation failed")
        
        # Test 2: Agent Initialization
        print("üß™ Test 2: Agent Initialization")
        try:
            model = create_nteo_model('degree_constrained', 4, num_commuters=20)
            
            commuter_count = model.schedule_commuters.get_agent_count()
            station_count = model.schedule_stations.get_agent_count()
            maas_count = model.schedule_maas.get_agent_count()
            
            assert commuter_count == 20
            assert station_count > 0
            assert maas_count > 0
            
            print(f"   ‚úÖ Agents: {commuter_count} commuters, {station_count} stations, {maas_count} MaaS")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Agent initialization failed: {e}")
            results['tests_failed'] += 1
        
        # Test 3: Single Routing System
        print("üß™ Test 3: Single Routing System Validation")
        try:
            model = create_nteo_model('degree_constrained', 4, num_commuters=20)
            
            # Verify only network routing exists
            assert hasattr(model, 'find_route')
            assert hasattr(model, 'network_interface')
            
            # Test route finding
            route = model.find_route((10, 10), (20, 20))
            # Route may be empty if no valid path, but function should not crash
            
            print("   ‚úÖ Single routing system working (dual system eliminated)")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Single routing system validation failed: {e}")
            results['tests_failed'] += 1
            results['critical_issues'].append("Single routing system validation failed")
        
        # Test 4: Simulation Steps
        print("üß™ Test 4: Simulation Execution")
        try:
            model = create_nteo_model('degree_constrained', 4, num_commuters=20)
            
            initial_route_count = model.route_calculation_count
            
            # Run a few simulation steps
            for i in range(5):
                model.step()
            
            # Verify simulation ran without errors
            assert model.route_calculation_count >= initial_route_count
            
            print(f"   ‚úÖ Simulation runs successfully (5 steps completed)")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Simulation execution failed: {e}")
            results['tests_failed'] += 1
        
        return results
    
    def _test_performance_metrics(self) -> Dict[str, Any]:
        """Test performance improvements from eliminating dual system"""
        
        results = {'tests_passed': 0, 'tests_failed': 0, 'critical_issues': [], 'performance_data': {}}
        
        print("üß™ Performance Validation - Testing for 40-60% improvement expectation")
        
        # Test 1: Network Creation Performance
        print("üß™ Test 1: Network Creation Performance")
        try:
            creation_times = []
            
            for i in range(3):
                start_time = time.time()
                network_interface = create_network('degree_constrained', 4)
                creation_time = time.time() - start_time
                creation_times.append(creation_time)
            
            avg_creation_time = np.mean(creation_times)
            results['performance_data']['network_creation_time'] = avg_creation_time
            
            print(f"   ‚úÖ Average network creation time: {avg_creation_time:.3f}s")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Network creation performance test failed: {e}")
            results['tests_failed'] += 1
        
        # Test 2: Route Calculation Performance
        print("üß™ Test 2: Route Calculation Performance")
        try:
            model = create_nteo_model('degree_constrained', 4, num_commuters=30)
            
            # Measure route calculation performance
            route_times = []
            for i in range(10):
                start_time = time.time()
                route = model.find_route((5, 5), (15, 15))
                route_time = time.time() - start_time
                route_times.append(route_time)
            
            avg_route_time = np.mean(route_times)
            results['performance_data']['route_calculation_time'] = avg_route_time
            
            print(f"   ‚úÖ Average route calculation time: {avg_route_time:.4f}s")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Route calculation performance test failed: {e}")
            results['tests_failed'] += 1
        
        # Test 3: Simulation Step Performance
        print("üß™ Test 3: Simulation Step Performance")
        try:
            model = create_nteo_model('degree_constrained', 4, num_commuters=50)
            
            # Measure simulation step performance
            step_times = []
            for i in range(10):
                start_time = time.time()
                model.step()
                step_time = time.time() - start_time
                step_times.append(step_time)
            
            avg_step_time = np.mean(step_times)
            results['performance_data']['simulation_step_time'] = avg_step_time
            
            print(f"   ‚úÖ Average simulation step time: {avg_step_time:.3f}s")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Simulation step performance test failed: {e}")
            results['tests_failed'] += 1
        
        # Test 4: Memory Usage Validation
        print("üß™ Test 4: Memory Usage Check")
        try:
            import psutil
            import os
            
            process = psutil.Process(os.getpid())
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # Create multiple models to test memory usage
            models = []
            for i in range(3):
                model = create_nteo_model('degree_constrained', 4, num_commuters=30)
                models.append(model)
            
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = final_memory - initial_memory
            
            results['performance_data']['memory_usage_mb'] = memory_increase
            
            print(f"   ‚úÖ Memory usage for 3 models: {memory_increase:.1f} MB")
            results['tests_passed'] += 1
            
        except ImportError:
            print("   ‚ö†Ô∏è Memory usage test skipped (psutil not available)")
        except Exception as e:
            print(f"   ‚ùå Memory usage test failed: {e}")
            results['tests_failed'] += 1
        
        return results
    
    def _test_research_framework(self) -> Dict[str, Any]:
        """Test the research framework functionality"""
        
        results = {'tests_passed': 0, 'tests_failed': 0, 'critical_issues': []}
        
        # Test 1: Research Runner Creation
        print("üß™ Test 1: Research Runner Creation")
        try:
            runner = NTEOResearchRunner("test_validation")
            assert hasattr(runner, 'run_single_topology_study')
            assert hasattr(runner, 'run_multi_topology_comparison')
            
            print("   ‚úÖ Research runner creates successfully")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Research runner creation failed: {e}")
            results['tests_failed'] += 1
            results['critical_issues'].append("Research runner creation failed")
        
        # Test 2: Single Topology Study (Mini)
        print("üß™ Test 2: Single Topology Study")
        try:
            runner = NTEOResearchRunner("test_validation")
            
            # Run very small study
            study_results = runner.run_single_topology_study(
                topology_type='degree_constrained',
                parameter_values=[3, 4],
                num_runs=1,
                steps_per_run=10,
                num_commuters=20
            )
            
            assert 'results' in study_results
            assert len(study_results['results']) == 2
            
            print("   ‚úÖ Single topology study runs successfully")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Single topology study failed: {e}")
            results['tests_failed'] += 1
        
        # Test 3: Research Study Configuration
        print("üß™ Test 3: Research Study Configuration")
        try:
            runner = NTEOResearchRunner("test_validation")
            
            # Test predefined research study
            research_results = runner.run_research_study(
                'degree_comparison_study',
                num_runs=1,
                steps_per_run=10,
                num_commuters=20
            )
            
            assert 'research_study_name' in research_results
            assert 'research_analysis' in research_results
            
            print("   ‚úÖ Research study configuration works")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Research study configuration failed: {e}")
            results['tests_failed'] += 1
        
        return results
    
    def _test_single_routing_system(self) -> Dict[str, Any]:
        """Test that dual routing system has been eliminated"""
        
        results = {'tests_passed': 0, 'tests_failed': 0, 'critical_issues': []}
        
        print("üß™ CRITICAL TEST: Dual Routing System Elimination")
        
        # Test 1: No Legacy Grid Routing References
        print("üß™ Test 1: Legacy Grid Routing Elimination")
        try:
            model = create_nteo_model('degree_constrained', 4, num_commuters=20)
            
            # Check that model only has network routing
            routing_methods = [attr for attr in dir(model) if 'route' in attr.lower()]
            
            # Should have find_route but not legacy routing methods
            assert hasattr(model, 'find_route')
            
            # Check for absence of legacy methods (adjust based on your legacy system)
            legacy_indicators = ['dijkstra_with_congestion', 'legacy_route', 'grid_route']
            for indicator in legacy_indicators:
                assert not hasattr(model, indicator), f"Legacy routing method {indicator} still present"
            
            print("   ‚úÖ No legacy routing methods detected")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Legacy routing elimination check failed: {e}")
            results['tests_failed'] += 1
            results['critical_issues'].append("Legacy routing system not fully eliminated")
        
        # Test 2: Single Route Calculation Path
        print("üß™ Test 2: Single Route Calculation Path")
        try:
            model = create_nteo_model('degree_constrained', 4, num_commuters=20)
            
            # Verify route calculation goes through network interface only
            initial_count = model.route_calculation_count
            
            # Call route finding
            route = model.find_route((5, 5), (15, 15))
            
            # Should increment count (indicating network routing used)
            assert model.route_calculation_count > initial_count
            
            print("   ‚úÖ Single route calculation path confirmed")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Single route calculation path test failed: {e}")
            results['tests_failed'] += 1
        
        # Test 3: Network-Only Congestion
        print("üß™ Test 3: Network-Only Congestion Model")
        try:
            model = create_nteo_model('degree_constrained', 4, num_commuters=30)
            
            # Run simulation to generate congestion
            for i in range(5):
                model.step()
            
            # Check congestion calculation uses network edges
            congestion_level = model._calculate_network_congestion()
            
            # Should be a valid number
            assert isinstance(congestion_level, (int, float))
            assert congestion_level >= 0
            
            print(f"   ‚úÖ Network congestion model working (level: {congestion_level:.3f})")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Network congestion model test failed: {e}")
            results['tests_failed'] += 1
        
        return results
    
    def _test_backward_compatibility(self) -> Dict[str, Any]:
        """Test backward compatibility for existing code"""
        
        results = {'tests_passed': 0, 'tests_failed': 0, 'critical_issues': []}
        
        # Test 1: Database Configuration Access
        print("üß™ Test 1: Database Configuration Backward Compatibility")
        try:
            # Test that existing database access patterns still work
            assert hasattr(db, 'NETWORK_CONFIG')
            assert hasattr(db, 'get_small_world_config')
            assert hasattr(db, 'get_degree_constrained_config')
            
            # Test functions return valid configurations
            sw_config = db.get_small_world_config(0.1)
            assert 'topology_type' in sw_config
            assert sw_config['topology_type'] == 'small_world'
            
            dc_config = db.get_degree_constrained_config(4)
            assert 'topology_type' in dc_config
            assert dc_config['topology_type'] == 'degree_constrained'
            
            print("   ‚úÖ Database configuration backward compatibility maintained")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Database backward compatibility failed: {e}")
            results['tests_failed'] += 1
        
        # Test 2: Network Configuration Update
        print("üß™ Test 2: Network Configuration Update Function")
        try:
            # Test global configuration update
            original_config = db.NETWORK_CONFIG.copy()
            
            success = db.update_network_configuration('small_world', rewiring_probability=0.2)
            assert success or True  # Should not crash
            
            print("   ‚úÖ Network configuration update function works")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Network configuration update failed: {e}")
            results['tests_failed'] += 1
        
        return results
    
    def _test_network_topologies(self) -> Dict[str, Any]:
        """Test different network topology generations"""
        
        results = {'tests_passed': 0, 'tests_failed': 0, 'critical_issues': []}
        
        # Test each topology type
        topology_tests = [
            ('degree_constrained', 4),
            ('small_world', 0.1),
            ('scale_free', 2),
            ('base_sydney', 6)
        ]
        
        for topology_type, param_value in topology_tests:
            print(f"üß™ Testing {topology_type} topology (param: {param_value})")
            
            try:
                network_interface = create_network(topology_type, param_value)
                stats = network_interface.get_network_stats()
                
                # Basic validation
                assert stats['num_nodes'] > 0, f"No nodes in {topology_type} network"
                assert stats['num_edges'] > 0, f"No edges in {topology_type} network"
                
                # Connectivity validation
                if stats['is_connected']:
                    print(f"   ‚úÖ {topology_type}: {stats['num_nodes']} nodes, {stats['num_edges']} edges, connected")
                else:
                    print(f"   ‚ö†Ô∏è {topology_type}: {stats['num_nodes']} nodes, {stats['num_edges']} edges, disconnected")
                
                results['tests_passed'] += 1
                
            except Exception as e:
                print(f"   ‚ùå {topology_type} topology failed: {e}")
                results['tests_failed'] += 1
                
                # Mark as critical if degree_constrained fails (baseline)
                if topology_type == 'degree_constrained':
                    results['critical_issues'].append(f"Baseline topology {topology_type} failed")
        
        return results
    
    def _test_data_consistency(self) -> Dict[str, Any]:
        """Test data consistency across the system"""
        
        results = {'tests_passed': 0, 'tests_failed': 0, 'critical_issues': []}
        
        # Test 1: Configuration Consistency
        print("üß™ Test 1: Configuration Data Consistency")
        try:
            # Create model with specific configuration
            model = create_nteo_model('degree_constrained', 4, num_commuters=25)
            
            # Verify configuration propagated correctly
            assert model.num_commuters == 25
            assert hasattr(model, 'network_interface')
            
            network_stats = model.network_interface.get_network_stats()
            assert network_stats['num_nodes'] > 0
            
            print("   ‚úÖ Configuration data consistent across system")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Configuration consistency test failed: {e}")
            results['tests_failed'] += 1
        
        # Test 2: Agent-Network Consistency
        print("üß™ Test 2: Agent-Network Data Consistency")
        try:
            model = create_nteo_model('degree_constrained', 4, num_commuters=20)
            
            # Verify agents are placed on valid grid coordinates
            commuter_agents = list(model.schedule_commuters.agents)
            
            for agent in commuter_agents[:5]:  # Test first 5 agents
                pos = agent.pos
                assert 0 <= pos[0] < model.grid.width
                assert 0 <= pos[1] < model.grid.height
            
            print("   ‚úÖ Agent placement consistent with grid dimensions")
            results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Agent-network consistency test failed: {e}")
            results['tests_failed'] += 1
        
        return results
    
    def _test_error_handling(self) -> Dict[str, Any]:
        """Test error handling and edge cases"""
        
        results = {'tests_passed': 0, 'tests_failed': 0, 'critical_issues': []}
        
        # Test 1: Invalid Topology Type
        print("üß™ Test 1: Invalid Topology Type Handling")
        try:
            try:
                network_interface = create_network('invalid_topology', 4)
                # Should not reach here
                results['tests_failed'] += 1
                print("   ‚ùå Invalid topology type not caught")
            except (ValueError, KeyError) as e:
                # Expected behavior
                print("   ‚úÖ Invalid topology type properly handled")
                results['tests_passed'] += 1
            
        except Exception as e:
            print(f"   ‚ùå Error handling test failed: {e}")
            results['tests_failed'] += 1
        
        # Test 2: Invalid Parameter Values
        print("üß™ Test 2: Invalid Parameter Value Handling")
        try:
            try:
                # Test with invalid parameter
                network_interface = create_network('small_world', -0.5)  # Invalid probability
                # May or may not fail depending on implementation
                print("   ‚ö†Ô∏è Invalid parameter accepted (may be valid behavior)")
                results['tests_passed'] += 1
                
            except (ValueError, AssertionError) as e:
                # Expected for invalid parameters
                print("   ‚úÖ Invalid parameter properly rejected")
                results['tests_passed'] += 1
                
        except Exception as e:
            print(f"   ‚ùå Parameter validation test failed: {e}")
            results['tests_failed'] += 1
        
        return results
    
    def _save_validation_results(self, results: Dict[str, Any]):
        """Save validation results to file"""
        
        output_file = self.output_dir / f"nteo_validation_{time.strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(output_file, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            print(f"\nüíæ Validation results saved to: {output_file}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Could not save validation results: {e}")

# ===== CONVENIENCE FUNCTIONS =====
def run_quick_validation():
    """Run a quick validation test"""
    validator = NTEOValidationSuite()
    
    # Run just critical tests
    critical_results = {
        'Configuration System': validator._test_configuration_system(),
        'Network Factory': validator._test_network_factory(),
        'Single Routing System': validator._test_single_routing_system()
    }
    
    print("\n" + "="*60)
    print("QUICK VALIDATION SUMMARY")
    print("="*60)
    
    total_passed = sum(r['tests_passed'] for r in critical_results.values())
    total_failed = sum(r['tests_failed'] for r in critical_results.values())
    critical_issues = []
    for r in critical_results.values():
        critical_issues.extend(r.get('critical_issues', []))
    
    print(f"‚úÖ Passed: {total_passed}")
    print(f"‚ùå Failed in run_quick_validation: {total_failed}")
    print(f"üö® Critical Issues: {len(critical_issues)}")
    
    if critical_issues:
        print("\nCritical Issues:")
        for issue in critical_issues:
            print(f"  - {issue}")
    
    success_rate = (total_passed / (total_passed + total_failed) * 100) if (total_passed + total_failed) > 0 else 0
    status = "PASS" if success_rate >= 80 and len(critical_issues) == 0 else "FAIL"
    
    print(f"\nOverall Status: {status} ({success_rate:.1f}% success rate)")
    
    return status == "PASS"

def run_performance_benchmark():
    """Run performance benchmark to validate improvements"""
    validator = NTEOValidationSuite()
    
    print("üöÄ NTEO Performance Benchmark")
    print("="*50)
    
    performance_results = validator._test_performance_metrics()
    
    if 'performance_data' in performance_results:
        data = performance_results['performance_data']
        
        print(f"üìä Performance Results:")
        print(f"  - Network Creation: {data.get('network_creation_time', 'N/A'):.3f}s")
        print(f"  - Route Calculation: {data.get('route_calculation_time', 'N/A'):.4f}s")
        print(f"  - Simulation Step: {data.get('simulation_step_time', 'N/A'):.3f}s")
        print(f"  - Memory Usage: {data.get('memory_usage_mb', 'N/A'):.1f} MB")
    
    return performance_results

# ===== MAIN EXECUTION =====
if __name__ == "__main__":
    print("NTEO Validation Suite")
    print("=" * 60)
    
    # Run quick validation first
    print("Running quick validation...")
    quick_success = run_quick_validation()
    
    if quick_success:
        print("\n‚úÖ Quick validation passed! Running full validation...")
        
        # Run full validation suite
        validator = NTEOValidationSuite()
        full_results = validator.run_full_validation()
        
        if full_results['overall_status'] == 'PASS':
            print("\nüéØ FULL VALIDATION SUCCESSFUL!")
            print("‚úÖ Unified NTEO system is ready for research use")
        else:
            print("\n‚ö†Ô∏è Full validation found issues - see detailed results")
    else:
        print("\n‚ùå Quick validation failed - system needs attention before full validation")